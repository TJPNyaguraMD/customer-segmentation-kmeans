{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Customer Segmentation Analysis: E-commerce Marketing Strategy\n",
    "\n",
    "### Leveraging K-Means Clustering for Targeted Customer Insights\n",
    "\n",
    "---\n",
    "\n",
    "##  Project Overview\n",
    "\n",
    "In this project, I perform a comprehensive customer segmentation analysis for a UK-based online retail company. By applying unsupervised machine learning techniques to transactional data, I aim to identify distinct customer groups based on their purchasing behavior.\n",
    "\n",
    "Understanding customer segments allows businesses to move from a \"one-size-fits-all\" marketing approach to personalized strategies that improve customer retention, increase average order value (AOV), and optimize marketing spend.\n",
    "\n",
    "##  Dataset Information\n",
    "\n",
    "The dataset used is the **Online Retail II** dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/502/online+retail+ii).\n",
    "\n",
    "* **Timeline:** Transactions occurring between 01/12/2009 and 09/12/2011.\n",
    "* **Context:** The company primarily sells unique all-occasion gift-ware. Many customers are wholesalers.\n",
    "* **Key Statistics:** * **Instances:** 525,461\n",
    "* **Features:** Invoice number, Stock code, Description, Quantity, Invoice date, Unit price, Customer ID, and Country.\n",
    "\n",
    "\n",
    "\n",
    "##  Business Objectives\n",
    "\n",
    "1. **Data Engineering:** Transform raw transactional data into a customer-centric **RFM (Recency, Frequency, Monetary)** framework.\n",
    "2. **Customer Understanding:** Perform Exploratory Data Analysis (EDA) to find trends in sales, geography, and top-selling products.\n",
    "3. **Machine Learning:** Implement a **K-Means Clustering** algorithm to segment customers.\n",
    "4. **Strategic Recommendations:** Interpret the clusters to provide actionable business recommendations for marketing and sales teams.\n",
    "\n",
    "## Technical Workflow\n",
    "\n",
    "1. **Data Cleaning:** Handling missing values (specifically Customer IDs) and removing cancellations/returns to ensure data integrity.\n",
    "2. **RFM Feature Engineering:**\n",
    "* **Recency:** Days since the last purchase.\n",
    "* **Frequency:** Total number of purchases.\n",
    "* **Monetary Value:** Total revenue generated by the customer.\n",
    "\n",
    "\n",
    "3. **Pre-processing:** Scaling features using `StandardScaler` to handle the variance in RFM metrics and addressing skewness.\n",
    "4. **Optimal Cluster Selection:** Utilizing the **Elbow Method** and **Silhouette Score** to determine the most effective number of clusters ().\n",
    "5. **Cluster Profiling:** Visualizing clusters using 3D scatter plots and snake plots to define \"Champions,\" \"At-Risk,\" and \"New Customers.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Key Skills Demonstrated:\n",
    "\n",
    "`Python` | `Pandas` | `Scikit-Learn` | `K-Means Clustering` | `RFM Analysis` | `Data Visualization (Matplotlib/Seaborn)` | `Exploratory Data Analysis (EDA)`\n",
    "\n"
   ],
   "id": "ca5335b7ce3aaf39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Import Necessary Packages \n",
   "id": "111db197cd7db9d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:13:18.397220Z",
     "start_time": "2026-01-30T14:13:18.382002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "# This turns the warning off globally\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import openpyxl\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "\n"
   ],
   "id": "aa332201736556fd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Dataset Injestion \n",
   "id": "cf1a1ae08bfbf0c0"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2026-01-30T14:13:22.206059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import io\n",
    "\n",
    "# Download the ZIP file\n",
    "url = 'https://archive.ics.uci.edu/static/public/502/online+retail+ii.zip'\n",
    "response = urllib.request.urlopen(url)\n",
    "zip_data = io.BytesIO(response.read())\n",
    "\n",
    "# Extract and read the Excel file from the ZIP\n",
    "with zipfile.ZipFile(zip_data) as zip_ref:\n",
    "    # List files in the archive to find the Excel file\n",
    "    file_list = zip_ref.namelist()\n",
    "    print(\"Files in archive:\", file_list)\n",
    "    \n",
    "    # Read the first Excel file (adjust the filename if needed)\n",
    "    excel_file = [f for f in file_list if f.endswith('.xlsx')][0]\n",
    "    with zip_ref.open(excel_file) as excel_data:\n",
    "        df = pd.read_excel(excel_data)\n",
    "\n",
    "df.head()"
   ],
   "id": "93481e8703e87367",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:13:08.512539Z",
     "start_time": "2026-01-30T14:13:08.512539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the dataset to file directory \n",
    "df.to_csv('Customer_understanding_kmeans.csv', index=False)"
   ],
   "id": "441f3bb59afbd8e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = pd.read_csv('Customer_understanding_kmeans.csv')",
   "id": "54110ca1a29aeb2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head()",
   "id": "f6c959b13017271d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Drop the column with unnamed\n",
    "df = df.drop(columns=['Unnamed: 0'])"
   ],
   "id": "d38685d10c338d0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Dataset Inspection ",
   "id": "35bc2373ccaa1a9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:13:08.516005Z",
     "start_time": "2026-01-30T14:13:08.516005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset inspection \n",
    "df.head()"
   ],
   "id": "532a4da9fa9d69d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dataset Info \n",
    "df.info()"
   ],
   "id": "bc9a1c6cd4c5bf0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:13:08.516005Z",
     "start_time": "2026-01-30T14:13:08.516005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define function that performs dataset inspection \n",
    "import pandas as pd\n",
    "\n",
    "def data_inspection(df):\n",
    "    \"\"\"\n",
    "    Performs a professional-grade inspection of the dataframe.\n",
    "    \"\"\"\n",
    "    print(\"=== Dataset Overview ===\")\n",
    "    print(f\"Total Rows: {df.shape[0]}\")\n",
    "    print(f\"Total Columns: {df.shape[1]}\")\n",
    "    print(f\"Duplicate Rows: {df.duplicated().sum()}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Create a summary table for columns\n",
    "    inspection_df = pd.DataFrame({\n",
    "        'Data Type': df.dtypes,\n",
    "        'Missing Values': df.isnull().sum(),\n",
    "        'Missing %': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "        'Unique Values': df.nunique()\n",
    "    })\n",
    "    \n",
    "    print(\"=== Column Statistics ===\")\n",
    "    display(inspection_df) # Use display() for a nice table in Jupyter\n",
    "    \n",
    "    print(\"\\n=== Sample Data (First 3 Rows) ===\")\n",
    "    display(df.head(3))\n",
    "\n",
    "data_inspection(df)"
   ],
   "id": "d34a86020832de0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The invoice date column is not in the correct datatype and I has to be changed to a date time format ",
   "id": "19a12cbbd6b7d8c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:13:08.516005Z",
     "start_time": "2026-01-30T14:13:08.516005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Change invoice date to date time format \n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])"
   ],
   "id": "1981d61bfa9feab0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:13:08.516005Z",
     "start_time": "2026-01-30T14:13:08.516005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the conversion \n",
    "df['InvoiceDate'].dtypes"
   ],
   "id": "b4e86f4f1dca8f39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Another column that needs converstion is the invoice column which should be the primary key and should be in an integer format since it is a numerical value . ",
   "id": "1eaa938782c4916e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T14:13:08.516005Z",
     "start_time": "2026-01-30T14:13:08.516005Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "43af4c83755c7fc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now to inspect high level Statistics ",
   "id": "825af30a145faa7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# high-level statistics for numerical features \n",
    "print(df.describe().T)"
   ],
   "id": "2fb3053ffa3cd733",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4e6ba6640d1b790d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Negative Quantity (-9600): These could represent cancelled orders or returns. Since K-Means clustering (RFM) calculates \"Total Spend,\" keeping these as-is might distort a customer's value.\n",
    "\n",
    "Negative Price (-53594.36): This is usually a \"bad debt\" adjustment or data entry error. A product cannot have a negative price in a standard retail transaction.\n",
    "\n",
    "Customer ID Missing (~108,000 rows): Since your goal is Customer Segmentation, you cannot cluster transactions that don't have a specific ID assigned to them.\n",
    "\n",
    "Skewed Data: Look at Quantity: The 75th percentile is 10, but the max is 19,152. This indicates massive outliers (likely wholesalers) that will pull your K-Means centroids away from the average user."
   ],
   "id": "47d107312dda239f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Inspect object columns \n",
    "print(df.describe(include ='O'))"
   ],
   "id": "686e9d90f89636f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Geographic Dominance: The United Kingdom appears 485,852 times out of 525,461. This means ~92% of your data is from one country.\n",
    "\n",
    "It could be useful to  focus segmentation on the UK market specifically to avoid \"noise\" from different shipping costs/currency behaviors in export markets.\n",
    "\n",
    "Product Variety: There are 4,632 unique stock codes. This is a high-cardinality feature.\n",
    "It should also be noted that there are 4681 descriptions this doesnt align with the stock codes and should be further investigated in analysis . \n",
    "\n",
    "Transaction Volume: There are 28,816 unique invoices. Since the total rows are over 525k, this confirms that each invoice contains multiple line items (average of ~18 items per basket)."
   ],
   "id": "a2b6f3c769152967"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inspect missing Values ",
   "id": "7a471261e6311961"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# inspect missing data \n",
    "df[df['Customer ID'].isna()].head(10)"
   ],
   "id": "19693f16ac0527e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It can be see that some of these values do not have positive quantity values and could this mean returns of the items. Since there are no customer id values this makes segmentation inneffective and thus conclude that this data is missing at random and the rows should be dropped . ",
   "id": "206441a8ae1393ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Inspect negative quantities \n",
    "df[df['Quantity'] <0].head(20)"
   ],
   "id": "a2be49fa1b18687b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Quite a large number of these negative values are occuring from the same invoice and as and were purchased at the same time . The inference could be that these purchases were returned goods . The C preceeding the invoice number indicates a cancellation of the the order . This should be investigated further . ",
   "id": "fb040c0ef14cb2c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Inspect invoice numbers \n",
    "df['Invoice'] =df['Invoice'].astype('str')\n",
    "# Use regular expression \n",
    "df[df['Invoice'].str.match('^\\\\d{6}$') == False]"
   ],
   "id": "caad601afa3aa58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It can be seen that 10 209 invoices resulted in cancellations of the purchase . ",
   "id": "238bcc91f61025e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# To see if C is the only unique character \n",
    "df['Invoice'].str.replace('[0-9]' ,'', regex=True).unique()"
   ],
   "id": "e9ad37b8c15b0659",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This indicates that there is invoices with the value A ",
   "id": "206a922f30e9040c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df[df['Invoice'].str.startswith(\"A\")]",
   "id": "3b416d9ff0c25115",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "These are 3 rows that indicate the descriptionis adjust bad debt that have negative pricing . These seem like accounting transactions. \n",
    "These values should be removed from the transactions. "
   ],
   "id": "76dde05a1e436277"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inspect Stock code column ",
   "id": "e47760f9ed95e35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Test stock code only contains 5 digit values \n",
    "# 1. Convert stockcode column to string\n",
    "df['StockCode'] = df['StockCode'].astype('str')\n",
    "# 2. Inspect values where stockcode is not only 5 digit values. \n",
    "df[df['StockCode'].str.match('^\\\\d{5}$') == False]"
   ],
   "id": "e6402b8d4d63a0c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "These transations are valid however it is not know what each stock code means ",
   "id": "3864d768a8c8f92d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df[(df['StockCode'].str.match('^\\\\d{5}$') == False) & (df['StockCode'].str.match('^\\\\d{5}[a-zA-Z]+$') == False)]",
   "id": "f7e8c5b93110147",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This shows that there are stock codes with the name for Postage , discount, and other codes that dont follow the specific pattern . ",
   "id": "a304a5ff1254b01f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df[(df['StockCode'].str.match('^\\\\d{5}$') == False) & (df['StockCode'].str.match('^\\\\d{5}[a-zA-Z]+$') == False)]['StockCode'].unique()",
   "id": "e04a50e2504c7dd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# To inspect these values \n",
    "\n",
    "values_to_inspect = ['POST', 'D', 'DCGS0058', 'DCGS0068', 'DOT', 'M', 'DCGS0004',\n",
    "       'DCGS0076', 'C2', 'BANK CHARGES', 'DCGS0003', 'TEST001',\n",
    "       'gift_0001_80', 'DCGS0072', 'gift_0001_20', 'DCGS0044', 'TEST002',\n",
    "       'gift_0001_10', 'gift_0001_50', 'DCGS0066N', 'gift_0001_30',\n",
    "       'PADS', 'ADJUST', 'gift_0001_40', 'gift_0001_60', 'gift_0001_70',\n",
    "       'gift_0001_90', 'DCGSSGIRL', 'DCGS0006', 'DCGS0016', 'DCGS0027',\n",
    "       'DCGS0036', 'DCGS0039', 'DCGS0060', 'DCGS0056', 'DCGS0059', 'GIFT',\n",
    "       'DCGSLBOY', 'm', 'DCGS0053', 'DCGS0062', 'DCGS0037', 'DCGSSBOY',\n",
    "       'DCGSLGIRL', 'S', 'DCGS0069', 'DCGS0070', 'DCGS0075', 'B',\n",
    "       'DCGS0041', 'ADJUST2', '47503J ', 'C3', 'SP1002', 'AMAZONFEE']\n"
   ],
   "id": "7f549d3e8ae09c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Data Cleaning ",
   "id": "652d20aa39407607"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# copy dataset for cleaning \n",
    "cleaned_df = df.copy()"
   ],
   "id": "d54cd0ab8640d719",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Clean Invoice Column ",
   "id": "94e0ee99dd5eb3c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cleaned_df",
   "id": "f43c2a42409ff160",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To s",
   "id": "8330cfddab5340f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cleaned_df['Invoice'] = cleaned_df['Invoice'].astype('str')\n",
    "mask = (\n",
    "    cleaned_df['Invoice'].str.match('^\\\\d{6}$') == True \n",
    ")\n",
    "\n",
    "cleaned_df = cleaned_df[mask]"
   ],
   "id": "515e1f53e678a7cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7b604d228340b18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Find matches by digits or digits and are followed by letter and pads stockcode\n",
    "cleaned_df[\"StockCode\"] = cleaned_df['StockCode'].astype('str')\n",
    "stock_mask = (\n",
    "    (cleaned_df['StockCode'].str.match('^\\\\d{5}$') == True)\n",
    "    | (cleaned_df['StockCode'].str.match('^\\\\d{5}[a-zA-Z]+$') == True)\n",
    "    | (cleaned_df['StockCode'].str.match('^PADS$') == True)\n",
    ")\n",
    "\n",
    "cleaned_df = cleaned_df[stock_mask]\n",
    "\n",
    "cleaned_df"
   ],
   "id": "984daa772fddfadf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We must deal with quantities less than zero drop missing values and also \n",
    "# Create a filter that removes rows ONLY if BOTH conditions are met\n",
    "\n",
    "cleaned_df.dropna(subset = ['Customer ID'], inplace = True)"
   ],
   "id": "6f65905163c12aa4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cleaned_df.describe()",
   "id": "ca7f78a9ef2c9f83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " There are prices that are equal to zero which will not be helpful in our customer analysis ",
   "id": "27cb2741112231c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cleaned_df[cleaned_df['Price']== 0]",
   "id": "e6e05c25e7ff9335",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# filter out price = 0 \n",
    "cleaned_df = cleaned_df[cleaned_df['Price']>0]"
   ],
   "id": "a350b48c1156a4ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To finalise the cleaning process is to see how much data has been lost \n",
   "id": "4addf709cbc278d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# calculate the percentage of remaining data \n",
    "pct_data_left = (len(cleaned_df) / len(df))* 100 \n",
    "pct_data_lost = 100 - pct_data_left\n",
    "print(f' After the cleaning process there remains {pct_data_left:.2f}% of the original data')\n",
    "print(f' The cleaning process removed {pct_data_lost:.2f} % of the data' )"
   ],
   "id": "681877affb9f6df3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Feature Engineering ",
   "id": "d24533e32c81bf31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This section will perform the following functions \n",
    "1. Sales Totals \n",
    "2. Aggregation of Data By Customer ID \n",
    "3. Computing Recency Value \n",
    "4. Computing Frequency Value \n",
    "5. Computing Monetary Value "
   ],
   "id": "9ccb8b8cad5118f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a new column in the cleaned dataframe to find the total amount per sale \n",
    "cleaned_df['Sales_total_pounds'] = cleaned_df['Quantity']* cleaned_df['Price']\n",
    "cleaned_df"
   ],
   "id": "538f4236e1ab64a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now to create a new dataframe that will be aggregated ",
   "id": "78b60296ace9405"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create an aggregated df that groups by Monetary Frequency and Recency value \n",
    "aggregated_df = cleaned_df.groupby('Customer ID', as_index = False).agg(\n",
    "    Monetary_value = ('Sales_total_pounds', \"sum\"),\n",
    "    Frequency = ('Invoice', 'nunique'),  # counts the number of unique invoices per customer id \n",
    "    Last_invoice_date = ('InvoiceDate', 'max')\n",
    "    \n",
    "    \n",
    ")\n",
    "aggregated_df.head(5)                                              "
   ],
   "id": "95e7f19d68b62a96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# For recency we have to look at the date the last invoice was made . \n",
    "max_invoice_date = aggregated_df['Last_invoice_date'].max()\n",
    "\n",
    "# Create new column called recency which calculates the days between the invoices\n",
    "aggregated_df['Recency']= (max_invoice_date - aggregated_df['Last_invoice_date']).dt.days\n",
    "# Inspection of the dataframe \n",
    "aggregated_df.head(5)"
   ],
   "id": "8cb9e1efe4ef8c8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Observe the distribution of these features to see if there are outliers that can influence the Kmeans clustering algorithm . ",
   "id": "e3d911fd2e73a9ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e4a8e5a3528414c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6be200e49f46bd27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Monetary value is skewed. THis is the same for Frequecy with outliers in these distributions. \n",
    "    \n",
    "Now we inspect for outliers "
   ],
   "id": "76b853c58be5cc0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the visual style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Monetary Value Boxplot\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.boxplot(y=aggregated_df['Monetary_value'], color='skyblue')\n",
    "plt.title('Monetary Value Outliers', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Monetary Value (Pounds)')\n",
    "\n",
    "# Plot 2: Frequency Boxplot\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.boxplot(y=aggregated_df['Frequency'], color='lightgreen')\n",
    "plt.title('Frequency Outliers', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Plot 3: Recency Boxplot\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot(y=aggregated_df['Recency'], color='salmon')\n",
    "plt.title('Recency Outliers', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Recency (Days)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "8bd85c85ea01ae3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Significant Skewness: Both Monetary Value and Frequency exhibit heavy right-skewness. A small percentage of customers contribute a disproportionately high amount of revenue and transaction volume.\n",
    "\n",
    "Extreme Outliers: We see customers with spend exceeding £300,000, while the median is significantly lower. These outliers will dominate distance-based algorithms like K-Means.\n",
    "\n",
    "Strategic Treatment: To ensure a meaningful segmentation, I will apply a Log Transformation to these features. This will \"compress\" the distance between the outliers and the average customers, making the data more normally distributed and suitable for clustering."
   ],
   "id": "d06353fc08bebc56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# separate the monetary outliers using interquartile range \n",
    "m_quartile1 =aggregated_df['Monetary_value'].quantile(0.25)\n",
    "m_quartile3 = aggregated_df['Monetary_value'].quantile(0.75)\n",
    "m_iqr = m_quartile3 - m_quartile1\n",
    "\n",
    "# Now to address the outliers \n",
    "monetary_outliers_df = aggregated_df[(aggregated_df['Monetary_value'] > (m_quartile3 + 1.5 * m_iqr))].copy()\n",
    "monetary_outliers_df.describe()"
   ],
   "id": "62a475e1833e39d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# separate the monetary outliers using interquartile range \n",
    "f_quartile1 =aggregated_df['Frequency'].quantile(0.25)\n",
    "f_quartile3 = aggregated_df['Frequency'].quantile(0.75)\n",
    "f_iqr = f_quartile3 - f_quartile1\n",
    "\n",
    "# Now to address the outliers \n",
    "frequency_outliers_df = aggregated_df[(aggregated_df['Frequency'] > (f_quartile3 + 1.5 * f_iqr))].copy()\n",
    "frequency_outliers_df.describe()"
   ],
   "id": "9c685a2f19e574e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The monetary value outliers and frequency outliers likely will have ovelap \n",
    "Ill have to look at the pure monetary and pure frequency outliers and overlapping outliers . "
   ],
   "id": "c9731fd442ce089c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create new dataframe to filter for values that are not outliers \n",
    "non_outliers_df = aggregated_df[(~aggregated_df.index.isin(monetary_outliers_df.index))& (~aggregated_df.index.isin(frequency_outliers_df.index))]\n",
    "non_outliers_df.describe()"
   ],
   "id": "39c18b1cf589b3a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now to visualise the non outlier data vs the outlier data ",
   "id": "c64a98dff791d872"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the visual style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a large figure to hold all 6 plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# --- TOP ROW: ORIGINAL DATA WITH EXTREME OUTLIERS ---\n",
    "plt.subplot(2, 3, 1)\n",
    "sns.boxplot(y=aggregated_df['Monetary_value'], color='skyblue')\n",
    "plt.title('Monetary Value Outliers')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "sns.boxplot(y=aggregated_df['Frequency'], color='lightgreen')\n",
    "plt.title('Frequency Outliers')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.boxplot(y=aggregated_df['Recency'], color='salmon')\n",
    "plt.title('Recency Outliers')\n",
    "\n",
    "# --- BOTTOM ROW: CLEANED DATA (NON-OUTLIERS) ---\n",
    "plt.subplot(2, 3, 4)\n",
    "sns.boxplot(y=non_outliers_df['Monetary_value'], color='skyblue')\n",
    "plt.title('Monetary Value Non Outliers Boxplot')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "sns.boxplot(y=non_outliers_df['Frequency'], color='lightgreen')\n",
    "plt.title('Frequency Non Outliers Boxplot')\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "sns.boxplot(y=non_outliers_df['Recency'], color='salmon')\n",
    "plt.title('Recency Non Outliers Boxplot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "ac7f3ccd46d15a59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After removing extreme outliers (using a method like the Interquartile Range or IQR), you can finally see the \"box\" and \"whiskers\" for the majority of your customers. For example, the Monetary Value now focuses on the £0 - £3,500 range, allowing the model to see differences between \"normal\" customers",
   "id": "7762217c071f1c2d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f60aa842f778a741"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "scatter = ax.scatter(non_outliers_df['Monetary_value'], non_outliers_df['Frequency'], non_outliers_df['Recency']) \n",
    "ax.set_xlabel('Monetary Value')\n",
    "ax.set_ylabel('Frequency Outliers')\n",
    "ax.set_zlabel('Recency')\n",
    "ax.set_title('3D Scatter Plot of Customer Data') \n",
    "plt.show()\n"
   ],
   "id": "2d87e0cafc73d577",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We notice the data is in different scales . This is an indication of the need to use a scaler since the K means algorithm is sensitive to scaling . \n",
    "The algorithm should treat each feature with equal weight. "
   ],
   "id": "5ad6d2bf95c68a86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "cfe6a082faed45ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Scaling the data",
   "id": "eba4b6067e891d55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(non_outliers_df[['Monetary_value', 'Frequency', 'Recency']])\n",
    "scaled_data"
   ],
   "id": "b6787465f40ae1b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scaled_data_df = pd.DataFrame(scaled_data, columns = ['Monetary_value', 'Frequency', 'Recency'], index = non_outliers_df.index)\n",
    "scaled_data_df.head()"
   ],
   "id": "729168cb66c459d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "scatter = ax.scatter(scaled_data_df['Monetary_value'], scaled_data_df['Frequency'], scaled_data_df['Recency']) \n",
    "ax.set_xlabel('Monetary Value')\n",
    "ax.set_ylabel('Frequency ')\n",
    "ax.set_zlabel('Recency')\n",
    "ax.set_title('3D Scatter Plot of Customer Data Scaled') \n",
    "plt.show()\n"
   ],
   "id": "b8aae8a0fbe9ae49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The data is now on comparable scales and is on the same distribution ",
   "id": "daaab52513878402"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. KMeans Clustering ",
   "id": "2c818ffeda32d69a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To determine the optimal clusters for this K means algorithm I shall utilise the Elbow method. ",
   "id": "36e98422a30f4820"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "max_k = 12\n",
    "\n",
    "inertia = [] \n",
    "silhouette_scores = []\n",
    "\n",
    "k_values = range(2, max_k+1)\n",
    "for k in k_values:\n",
    "    \n",
    "    # fit predict kmeans clustering \n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, max_iter=1000)\n",
    "     \n",
    "    cluster_labels = kmeans.fit_predict(scaled_data_df)\n",
    "    \n",
    "    sil_score = silhouette_score(scaled_data_df, cluster_labels)\n",
    "    # append to lists \n",
    "    inertia.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(sil_score)\n",
    " \n",
    "# plotting sihouette scores and elbow method \n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# plotting elbow method subplot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_values, inertia, marker='o')\n",
    "plt.xticks(k_values, k_values)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Intertia')\n",
    "plt.title('KMeans Inertia for Different k values', fontweight='bold')\n",
    "\n",
    "# plotting silhouette subplot\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(k_values, silhouette_scores, marker='o', color = 'red')\n",
    "plt.title('Silhouette Scores for Different k values', fontweight='bold')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "    \n",
    "    "
   ],
   "id": "f50a8a50f3566e2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The inertia value rapidly decreases and we look for where the gradient decreases . The elbow has a gradient decrease between 4 and 5 clusters . To choose betweeen the two clusters I shall use the Silhouette score . The higher score is what we want between the two values and the curve on the right indicates that the cluster of 4 has a better score. In conclusion I shall utilise a k of 4 clusters. ",
   "id": "999809552c59e1a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Final Model ",
   "id": "727a63c26c1b1ba9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fitting the model with the correct parameters \n",
    "kmeans_final = KMeans(n_clusters=4, random_state=42, max_iter=1000) \n",
    "\n",
    "cluster_labels = kmeans_final.fit_predict(scaled_data_df)\n",
    "\n",
    "cluster_labels"
   ],
   "id": "a92577b6eea371d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Add clusters to original non outliers dataframe \n",
    "# The syntax is .loc[all_rows, 'new_column_name']\n",
    "non_outliers_df.loc[:, 'Cluster'] = cluster_labels\n",
    "non_outliers_df"
   ],
   "id": "38d0a120a739d011",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3260f2b1a537eced",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Now to visualise the different clusters on a 3d scatterplot ",
   "id": "4252795ca9d1673a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Define  professional color palette for the segments\n",
    "# Using standard Hex codes for consistency\n",
    "cluster_colors = {\n",
    "    0: '#1f77b4',  # Blue\n",
    "    1: '#ff7f0e',  # Orange\n",
    "    2: '#2ca02c',  # Green\n",
    "    3: '#d62728'   # Red\n",
    "}\n",
    "\n",
    "# 2. Map the clusters to their respective colors\n",
    "colors = non_outliers_df['Cluster'].map(cluster_colors)\n",
    "\n",
    "# 3. Initialize the 3D plot\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "# 4. Create the scatter plot\n",
    "# Using your specific RFM columns from the aggregated dataframe\n",
    "scatter = ax.scatter(\n",
    "    non_outliers_df['Monetary_value'],\n",
    "    non_outliers_df['Frequency'],\n",
    "    non_outliers_df['Recency'],\n",
    "    c=colors,           # Use the mapped solid colors\n",
    "    marker='o',         # Spherical markers\n",
    "    alpha=0.6,          # Slight transparency to see overlapping points\n",
    "    edgecolors='w',     # White edges for better point separation\n",
    "    s=60                # Marker size\n",
    ")\n",
    "\n",
    "# 5. Set labels and title\n",
    "ax.set_xlabel('Monetary Value (£)', labelpad=10, fontweight='bold')\n",
    "ax.set_ylabel('Frequency (Orders)', labelpad=10, fontweight='bold')\n",
    "ax.set_zlabel('Recency (Days)', labelpad=10, fontweight='bold')\n",
    "ax.set_title('3D Customer Segmentation: RFM Clusters', fontsize=16, pad=20)\n",
    "\n",
    "# 6. Add a legend to make it professional\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label=f'Cluster {i}',\n",
    "                          markerfacecolor=color, markersize=10) \n",
    "                   for i, color in cluster_colors.items()]\n",
    "ax.legend(handles=legend_elements, title=\"Customer Segments\", loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "6ee5eb6ef8547e99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Violin plots to better understand the distribution of the individual clusters. ",
   "id": "29caaa46dbdba61d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure Cluster is integer to match palette keys\n",
    "non_outliers_df['Cluster'] = non_outliers_df['Cluster'].astype(int)\n",
    "\n",
    "# Professional color palette\n",
    "cluster_palette = {0: '#1f77b4', 1: '#ff7f0e', 2: '#2ca02c', 3: '#d62728'}\n",
    "\n",
    "plt.figure(figsize=(12, 18))\n",
    "\n",
    "# Plot 1: Monetary Value by Cluster\n",
    "plt.subplot(3, 1, 1)\n",
    "sns.violinplot(\n",
    "    x='Cluster', \n",
    "    y='Monetary_value', \n",
    "    data=non_outliers_df, \n",
    "    palette=cluster_palette, \n",
    "    hue='Cluster',      # Fix: Assigning the x variable to hue\n",
    "    legend=False        # Fix: Removing the redundant legend\n",
    ")\n",
    "plt.title('Monetary Value by Cluster', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 2: Frequency by Cluster\n",
    "plt.subplot(3, 1, 2)\n",
    "sns.violinplot(\n",
    "    x='Cluster', \n",
    "    y='Frequency', \n",
    "    data=non_outliers_df, \n",
    "    palette=cluster_palette, \n",
    "    hue='Cluster', \n",
    "    legend=False\n",
    ")\n",
    "plt.title('Frequency by Cluster', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 3: Recency by Cluster\n",
    "plt.subplot(3, 1, 3)\n",
    "sns.violinplot(\n",
    "    x='Cluster', \n",
    "    y='Recency', \n",
    "    data=non_outliers_df, \n",
    "    palette=cluster_palette, \n",
    "    hue='Cluster', \n",
    "    legend=False\n",
    ")\n",
    "plt.title('Recency by Cluster', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "7f1bd55b2ee92744",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The violin plots show the distribution of Recency, Frequency, and Monetary value for each cluster. Here is the interpretation of that data based on your results:\n",
    "\n",
    "Monetary Value by Cluster: Identifies \"Big Spenders\" (Cluster 3) versus those who make smaller, occasional purchases (Cluster 1).\n",
    "\n",
    "Frequency by Cluster: Helps distinguish loyal, repeat customers (Cluster 3) from one-time or infrequent shoppers.\n",
    "\n",
    "Recency by Cluster: Crucial for identifying \"at-risk\" customers (Cluster 1) who haven't visited in a long time compared to recent shoppers (Clusters 0, 2, and 3)."
   ],
   "id": "fcf90e21586e7723"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "##  Cluster 3: The Champions (The High-Value Segment)\n",
    "\n",
    "These are your best customers. They represent the \"Gold Standard\" of your retail business.\n",
    "\n",
    "* **Data Profile:** Highest **Monetary Value** (they spend the most) and highest **Frequency** (they shop the most often). Their **Recency** is very low, meaning they have made a purchase very recently.\n",
    "* **Psychology:** They have a high degree of trust in the brand and likely consider you their \"go-to\" for gift-ware.\n",
    "* ** Strategy:** \"Maintain and Reward.\" Do not spam them with generic discounts. Instead, offer them exclusive early access to new collections or a dedicated account manager.\n",
    "\n",
    "##  Cluster 0: Potential Loyalists (The Growth Segment)\n",
    "\n",
    "These are your rising stars. They are active and spending well, but they haven't reached \"Champion\" status yet.\n",
    "\n",
    "* **Data Profile:** Low **Recency** (recently active) and moderate-to-high **Monetary Value**. Their **Frequency** is stable but lower than Cluster 3.\n",
    "* **Psychology:** They are currently engaged with your brand and are in the \"habit-forming\" stage of their customer journey.\n",
    "* ** Strategy:** \"Upsell and Cross-sell.\" Use their recent purchase history to recommend complementary products. The goal is to increase their \"Frequency\" to move them into Cluster 3. These may also benefit from loyalty programs \n",
    "\n",
    "##  Cluster 2: New or Occasional Shoppers (The Entry Segment)\n",
    "\n",
    "This group is often the largest in terms of raw customer count.\n",
    "\n",
    "* **Data Profile:** Very low **Frequency** (often just 1 or 2 orders) and low **Monetary Value**. However, their **Recency** is low, meaning they have visited recently.\n",
    "* **Psychology:** They are either brand new to the store or only shop for specific, infrequent occasions.\n",
    "* ** Strategy:** \"Nurture and Educate.\" Focus on \"Welcome\" email sequences. Provide them with \"Social Proof\" (reviews/testimonials) to build the trust necessary for a second or third purchase.\n",
    "\n",
    "## Cluster 1: At-Risk / Hibernating (The Churn Segment)\n",
    "\n",
    "This is your \"Warning\" group. These customers were once active but have gone cold.\n",
    "\n",
    "* **Data Profile:** Highest **Recency** (it has been a long time since their last purchase). Their **Monetary** and **Frequency** values are typically low to moderate.\n",
    "* **Psychology:** They may have switched to a competitor, had a bad experience, or simply forgotten about the brand.\n",
    "* ** Strategy:** \"Re-activate or Let Go.\" This is the only group where aggressive discounting is justified. Send a \"We Miss You\" campaign with a 20% discount to see if they can be moved back into the \"Potential Loyalist\" group.\n",
    "\n",
    "---\n"
   ],
   "id": "36f48eccca3c18fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "668db5ea84498d06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Monetary and Frequency Outliers ",
   "id": "12e5e5ce3f1cca65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# --- STEP: Outlier Categorization & Segment Isolation ---",
   "id": "e5bfaf7e9d75b00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# We identify the overlap to avoid double-counting customers who are outliers in both metrics\n",
    "overlap_indices = monetary_outliers_df.index.intersection(frequency_outliers_df.index)\n",
    "\n",
    "# Isolate customers who are outliers ONLY in their spending (Monetary)\n",
    "monetary_only_outliers = monetary_outliers_df.drop(overlap_indices)\n",
    "\n",
    "# Isolate customers who are outliers ONLY in their purchase regularity (Frequency)\n",
    "frequency_only_outliers = frequency_outliers_df.drop(overlap_indices)\n",
    "\n",
    "# Identify the customers who are extreme outliers in both Spend and Frequency\n",
    "monetary_and_frequency_outliers = monetary_outliers_df.loc[overlap_indices]"
   ],
   "id": "65dbab6018593d36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# --- Manual Labeling Strategy ---"
   ],
   "id": "95985d531bc54d9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# We assign negative cluster IDs to distinguish these from the K-Means generated clusters (0, 1, 2, 3).\n",
    "# This allows us to keep the extreme data points accessible for separate high-value marketing analysis.\n",
    "monetary_only_outliers['Cluster'] = -1             # Flag for extreme high-spenders\n",
    "frequency_only_outliers['Cluster'] = -2            # Flag for extreme high-frequency shoppers\n",
    "monetary_and_frequency_outliers['Cluster'] = -3    # Flag for elite 'top-tier' outliers (both)\n",
    "\n",
    "# Unified outlier repository for specialized business reporting\n",
    "outlier_cluster_df = pd.concat([\n",
    "    monetary_only_outliers, \n",
    "    frequency_only_outliers, \n",
    "    monetary_and_frequency_outliers\n",
    "])\n",
    "outlier_cluster_df"
   ],
   "id": "7d30a394db2cb6bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Define a high-contrast palette for the outliers to distinguish them from standard clusters\n",
    "# -1: Monetary Outliers, -2: Frequency Outliers, -3: Both\n",
    "cluster_colors2 = {-1: '#9467bd', -2: '#8c564b', -3: '#e377c2'}\n",
    "\n",
    "plt.figure(figsize=(12, 18))\n",
    "\n",
    "# --- Plot 1: Monetary Value Distribution in Outliers ---\n",
    "plt.subplot(3, 1, 1)\n",
    "# Ensure you use the correct column name from your dataframe (e.g., 'Monetary_value' or 'MonetaryValue')\n",
    "sns.violinplot(\n",
    "    x='Cluster', \n",
    "    y='Monetary_value', \n",
    "    data=outlier_cluster_df, \n",
    "    palette=cluster_colors2,\n",
    "    hue='Cluster',\n",
    "    legend=False\n",
    ")\n",
    "plt.title('Monetary Value By Cluster', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Monetary Value (£)')\n",
    "\n",
    "# --- Plot 2: Frequency Distribution in Outliers ---\n",
    "plt.subplot(3, 1, 2)\n",
    "sns.violinplot(\n",
    "    x='Cluster', \n",
    "    y='Frequency', \n",
    "    data=outlier_cluster_df, \n",
    "    palette=cluster_colors2,\n",
    "    hue='Cluster',\n",
    "    legend=False\n",
    ")\n",
    "plt.title('Frequency Value By Cluster', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Frequency (Orders)')\n",
    "\n",
    "# --- Plot 3: Recency Distribution in Outliers ---\n",
    "plt.subplot(3, 1, 3)\n",
    "sns.violinplot(\n",
    "    x='Cluster', \n",
    "    y='Recency', \n",
    "    data=outlier_cluster_df, \n",
    "    palette=cluster_colors2,\n",
    "    hue='Cluster',\n",
    "    legend=False\n",
    ")\n",
    "plt.title('Recency Value By Cluster', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Recency By Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "7753833342539289",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  Strategic Outlier Summary\n",
    "\n",
    "The analysis isolated three distinct \"Manual Clusters\" for customers who exceeded the standard interquartile range (IQR) for spend or frequency:\n",
    "\n",
    "* **Cluster -1 (Monetary Outliers) — PAMPER**:\n",
    "* **Characteristics**: These are high spenders who make large but infrequent purchases.\n",
    "* **Potential Strategy**: Focus on maintaining loyalty through personalized offers or luxury services that cater specifically to their high spending capacity.\n",
    "\n",
    "\n",
    "* **Cluster -2 (Frequency Outliers) — UPSELL**:\n",
    "* **Characteristics**: These are consistently engaged, frequent buyers who spend less per individual purchase.\n",
    "* **Potential Strategy**: Implement loyalty programs or bundle deals to encourage higher spending per visit, leveraging their existing high engagement.\n",
    "\n",
    "\n",
    "* **Cluster -3 (Monetary & Frequency Outliers) — DELIGHT**:\n",
    "* **Characteristics**: Representing the most valuable \"Elite\" outliers, these customers exhibit both extreme spending and extreme purchase frequency.\n",
    "* **Potential Strategy**: Develop dedicated VIP programs or exclusive offers to maintain their high-tier loyalty and encourage continued engagement.\n",
    "\n"
   ],
   "id": "438abc8bf8e1b5ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now to combine all our clusters into a single dataframe \n",
    "#1. Create a dictionary of cluster labels \n",
    "\n",
    "cluster_labels_final = { \n",
    "    0: 'Core Regulars',    # Was Retain\n",
    "    1: 'Win-back Opportunity', # Was Re-engage\n",
    "    2: 'Promising Leads',  # Was Nurture\n",
    "    3: 'Loyalty Members',  # Was Reward\n",
    "   -1:'VIP Elites',       # Was Pamper (Cluster -1)\n",
    "   -2: 'High-Volume Growth', # Was Upsell (Cluster -2)\n",
    "   -3: 'Platinum Whales'   # Was Delight (Cluster -3)\n",
    "}"
   ],
   "id": "768bc316363fe23e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "final_cluster_df = pd.concat([non_outliers_df, outlier_cluster_df])\n",
    "final_cluster_df"
   ],
   "id": "85a1288e96a703ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now to map the specific clusters into the dataframe \n",
    "final_cluster_df['Cluster_label'] = final_cluster_df['Cluster'].map(cluster_labels_final)\n",
    "final_cluster_df\n"
   ],
   "id": "a2f95abcc3f59585",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visualise results \n",
    "\n"
   ],
   "id": "e11fc60d5f5bef1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate counts and feature means\n",
    "cluster_counts = final_cluster_df['Cluster_label'].value_counts()\n",
    "feature_means = final_cluster_df.groupby('Cluster_label')[['Recency', 'Frequency', 'Monetary_value']].mean()\n",
    "\n",
    "# Initialize the plot\n",
    "fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# 1. Bar Plot: Number of Customers per Segment\n",
    "sns.barplot(x=cluster_counts.index, y=cluster_counts.values, ax=ax1, palette='Spectral', hue=cluster_counts.index, legend=False)\n",
    "ax1.set_ylabel('Number of Customers', color='b', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Final Customer Segment Distribution & Average Metrics', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# 2. Line Plot: Feature Means (Dual Axis)\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(data=feature_means, ax=ax2, palette='crest', marker='o', linewidth=2.5, markersize=10)\n",
    "ax2.set_ylabel('Average Metric Value', color='g', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "677cb9f7d1a7051d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This dual-axis chart effectively communicates the balance between customer volume and business value.\n",
    "\n",
    "Customer Volume (Bars): The segments on the left represent the majority of the customer base. \"Promising Leads\" is the largest group, followed by \"Core Regulars\" and \"Win-back Opportunities.\" This indicates a large pool of potential value that requires nurturing and reactivation.\n",
    "\n",
    "Average Metric Value (Line Plots): The lines represent the average Monetary Value, Frequency, and Recency for each group.\n",
    "\n",
    "Platinum Whales: While this group has a smaller customer count, the blue dashed line spikes significantly, showing that these individuals contribute the highest individual monetary value to the business.\n",
    "\n",
    "VIP Elites & High-Volume Growth: These segments also show elevated monetary contributions compared to the core retail groups, despite their smaller population sizes.\n",
    "\n",
    "Frequency and Recency: These metrics remain relatively stable across most segments, but the sharp contrast in Monetary Value (blue line) clearly distinguishes the high-value \"Whales\" from the general population."
   ],
   "id": "5123b962785c624"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Project Conclusion: Customer Segmentation of Online Retail II\n",
    "\n",
    "This project successfully transformed a raw dataset of over one million transactions into a strategic roadmap for business growth. By leveraging **RFM Analysis** and **K-Means Clustering**, we moved beyond basic data exploration to discover actionable customer personas.\n",
    "\n",
    "---\n",
    "\n",
    "###  Key Achievements\n",
    "\n",
    "* **Data Integrity & Preprocessing:** Addressed significant data quality issues by handling missing Customer IDs and filtering out transaction anomalies like returns (negative quantities) and debt adjustments (negative prices).\n",
    "* **Strategic Outlier Management:** Rather than simply deleting extreme data points, \"Whale\" customers were isolated into manual segments (Monetary, Frequency, and Dual Outliers). This protected the integrity of the K-Means model while preserving the highest-value accounts for specialized B2B handling.\n",
    "* **Optimal Segment Discovery:** Utilizing the Elbow Method and Silhouette Scores, we identified  as the mathematically optimal number of clusters for the core customer base.\n",
    "* **Multidimensional Insight:** Through 3D Scatter Plots and Violin Plots, we validated that each cluster represents a distinct behavioral pattern across Recency, Frequency, and Monetary value.\n",
    "\n",
    "---\n",
    "\n",
    "###  Final Business Impact\n",
    "\n",
    "The true value of this analysis lies in its application to marketing and operations:\n",
    "\n",
    "* **Precision Marketing:** The business can now replace \"mass-blasting\" emails with targeted campaigns, such as **Loyalty Rewards for Champions** (Cluster 3) and **Reactivation Discounts for At-Risk customers** (Cluster 1).\n",
    "* **Resource Allocation:** By identifying **Potential Loyalists** (Cluster 0), the marketing team can focus acquisition and upselling efforts on the group most likely to become the next generation of high-value shoppers.\n",
    "* **VIP Retention:** The separate profiling of **Dual Outliers** ensures that the top 1% of revenue-generating accounts receive the high-touch service or wholesale pricing required to prevent churn.\n"
   ],
   "id": "ab616690adf8cfb9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Project Conclusion: Customer Segmentation of Online Retail II\n",
    "\n",
    "This project successfully transformed over one million raw transactions from the **Online Retail II** dataset into a high-impact, customer-centric segmentation strategy. By combining unsupervised machine learning (-Means) with advanced outlier profiling, I identified seven distinct customer groups that enable data-driven marketing decisions.\n",
    "\n",
    "---\n",
    "\n",
    "###  Key Methodological Achievements\n",
    "\n",
    "* **Data Integrity:** Implemented a robust cleaning pipeline to handle negative quantities (returns), missing customer identifiers, and non-product stock codes like \"POSTAGE,\" ensuring the final analysis was based on genuine purchase behavior.\n",
    "* **Strategic Outlier Management:** Utilizing the Interquartile Range (IQR) method, I isolated extreme \"Whales\" (Clusters -1, -2, and -3) before training the model. This prevented the centroids from distorting and allowed for a highly accurate -Means model for the core retail population.\n",
    "* **Mathematical Validation:** Optimized the clustering model () through the **Silhouette Score** and **Elbow Method**, validating that the resulting segments were statistically significant and well-separated in 3D space.\n",
    "\n",
    "---\n",
    "\n",
    "###  Business Impact and Behavioral Insights\n",
    "\n",
    "#### 1. The Core Retail Segments (Clusters 0 to 3)\n",
    "\n",
    "The analysis identified four key archetypes within the standard customer base:\n",
    "\n",
    "* **Champions (Cluster 3):** The highest spenders and most frequent visitors. **Strategy:** VIP loyalty rewards.\n",
    "* **Potential Loyalists (Cluster 0):** High recency and growing spend. **Strategy:** Cross-selling to increase purchase frequency.\n",
    "* **Recent/Occasional (Cluster 2):** New shoppers with low frequency but high engagement. **Strategy:** Personalized welcome sequences.\n",
    "* **At-Risk (Cluster 1):** Customers who have not visited in over 6 months. **Strategy:** Re-engagement campaigns with win-back discounts.\n",
    "\n",
    "#### 2. The Elite Outlier Profiles (Clusters -1 to -3)\n",
    "\n",
    "By profiling the outliers separately, I uncovered critical B2B and \"Super-Shopper\" segments:\n",
    "\n",
    "* **PAMPER (Cluster -1):** High-ticket, infrequent spenders. **Focus:** Maintaining loyalty with luxury service.\n",
    "* **UPSELL (Cluster -2):** Highly frequent buyers with **low monetary value** per order. **Focus:** Encouraging bundle deals and larger orders to optimize shipping costs.\n",
    "* **DELIGHT (Cluster -3):** The most valuable \"Dual Outliers\" with extreme spend and frequency. **Focus:** Dedicated VIP account management.\n",
    "\n",
    "---\n",
    "\n",
    "###  Final Summary\n",
    "\n",
    "This analysis proves that a one-size-fits-all marketing approach is inefficient. By leveraging **Violin Plots** to understand data density and **-Means** to define behavior, this project provides the business with a roadmap to maximize **Customer Lifetime Value (CLV)**. The distinction between \"Whales\" and \"Retail Shoppers\" ensures that marketing resources are allocated where they will generate the highest return on investment.\n",
    "\n",
    "\n",
    "###  Technical Stack\n",
    "\n",
    "`Python` | `Pandas` | `Scikit-Learn (K-Means)` | `Matplotlib` | `Seaborn` | `RFM Feature Engineering`\n",
    "\n",
    "---\n"
   ],
   "id": "35b0bd27bd26cafb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c58b9d2614ac3f7e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
